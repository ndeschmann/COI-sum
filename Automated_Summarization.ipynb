{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndeschmann/COI-sum/blob/main/Automated_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FerkV5Lsw7F"
      },
      "outputs": [],
      "source": [
        "#Install libraries\n",
        "%pip install transformers\n",
        "%pip install datasets\n",
        "%pip install torch\n",
        "%pip install pandas\n",
        "%pip install scikit-learn\n",
        "%pip install rouge\n",
        "%pip install beautifulsoup4\n",
        "%pip install sentencepiece\n",
        "%pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_OuZ4pqxCYL"
      },
      "outputs": [],
      "source": [
        "#Import libraries\n",
        "import transformers\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS9A2oM_teDU",
        "outputId": "5131f1a6-5413-4548-d5b5-d39f868e3089"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMoctfRYFgaZ"
      },
      "source": [
        "#Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02BNZHw5Fcyr"
      },
      "outputs": [],
      "source": [
        "def decode_json(s, _w=json.decoder.WHITESPACE.match):\n",
        "    decoder = json.JSONDecoder()\n",
        "    pos = 0\n",
        "    while pos < len(s):\n",
        "        try:\n",
        "            obj, pos = decoder.raw_decode(s, pos)\n",
        "            yield obj\n",
        "        except json.JSONDecodeError as e:\n",
        "            # Tries to find the next '{' to continue decoding\n",
        "            brace_pos = s.find('{', pos)\n",
        "            if brace_pos == -1:\n",
        "                break\n",
        "            pos = brace_pos + 1\n",
        "\n",
        "def extract_info(entry):\n",
        "    try:\n",
        "        content_html = entry.get('content', '')\n",
        "        description_en = entry.get('description_en', '')\n",
        "\n",
        "        # Removing HTML tags and formatting from the content\n",
        "        content_text = BeautifulSoup(content_html, 'html.parser').get_text()\n",
        "\n",
        "        # Removing backslash tags and additional formatting\n",
        "        content_text = re.sub(r'\\\\[^\\s]+', '', content_text)\n",
        "        content_text = re.sub(r'\\s+', ' ', content_text).strip()\n",
        "\n",
        "        # Storing the extracted information in a dictionary\n",
        "        extracted_info = {\n",
        "            \"content_text\": content_text,\n",
        "            \"description_en\": description_en\n",
        "        }\n",
        "\n",
        "        return extracted_info\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "        return None\n",
        "\n",
        "extracted_data = []\n",
        "\n",
        "with open('/content/drive/MyDrive/ACCORD_Summarisation/docv_10k_post202208.json', 'r', encoding='utf-8') as json_file:\n",
        "    for entry_or_list in decode_json(json_file.read()):\n",
        "        if not entry_or_list:\n",
        "            continue\n",
        "\n",
        "        # If it's a list, process each entry in the list\n",
        "        if isinstance(entry_or_list, list):\n",
        "            for entry in entry_or_list:\n",
        "                extracted_info = extract_info(entry)\n",
        "                if extracted_info:\n",
        "                    extracted_data.append(extracted_info)\n",
        "        else:\n",
        "            extracted_info = extract_info(entry_or_list)\n",
        "            if extracted_info:\n",
        "                extracted_data.append(extracted_info)\n",
        "\n",
        "with open('/content/drive/MyDrive/ACCORD_Summarisation/docv_10k_clean.json', 'w', encoding='utf-8') as output_file:\n",
        "    json.dump(extracted_data, output_file, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_rI0ruz2qLA"
      },
      "source": [
        "#Preprocessing and Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKfVvNYQAyYc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/ACCORD_Summarisation/docv_10k_clean.json\"\n",
        "\n",
        "dataset = load_dataset('json', data_files=file_path, split='train')\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return {\n",
        "        'input_text': examples['content_text'],\n",
        "        'target_text': examples['description_en']\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(preprocess_function)\n",
        "\n",
        "train_test_split = dataset.train_test_split(test_size=0.2)\n",
        "test_val_split = train_test_split['test'].train_test_split(test_size=0.5)\n",
        "\n",
        "split_datasets = DatasetDict({\n",
        "    'train': train_test_split['train'],\n",
        "    'validation': test_val_split['train'],\n",
        "    'test': test_val_split['test']\n",
        "})\n",
        "\n",
        "split_datasets.save_to_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_10k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPcHPqzG0qVk"
      },
      "source": [
        "#First Model Training:\n",
        "#**Fine-tune Training BART CNN**\n",
        "\n",
        "https://huggingface.co/facebook/bart-large-cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTdCnke7SiCo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "\n",
        "split_datasets = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_10k\")\n",
        "\n",
        "model_name = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenizing the datasets\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples['input_text'], max_length=1024, truncation=True, padding='max_length')\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples['target_text'], max_length=128, truncation=True, padding='max_length')\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = split_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "# Defining the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN_10k',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN_10k/logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=4\n",
        ")\n",
        "\n",
        "# Initializing the Data Collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Initializing the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model('/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN_10k')\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "results = trainer.evaluate(tokenized_datasets['test'])\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMWKZQPNWCPr"
      },
      "outputs": [],
      "source": [
        "#Saving the tokenizer\n",
        "from datasets import load_from_disk\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "model_name = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/ACCORD_Summarisation/tokenizer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx3TY66P1mMz"
      },
      "source": [
        "##Testing the Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9ky-8BV49OI",
        "outputId": "c108de1b-d52a-4254-b76a-400d7fa86c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text: Colombia’s Path to “Total Peace”. President Gustavo Petro cannot fall back on the FARC blueprint NO. 54 SEPTEMBER 2022 Introduction Colombia’s Path to “Total Peace” President Gustavo Petro cannot fall back on the FARC blueprint Günther Maihold With their joint announcement about the desire to resume peace talks, Colombia’s new president and the country’s second-largest guerrilla group, the ELN (Ejército de Liberación Nacional), have sent a clear political signal. The pacification of the ELN is to take place under the aegis of a “leftist” government and be accompanied by a com- prehensive and ambitious reform project. This is a renewed attempt to end the civil war following the conclusion of a peace agreement with the FARC rebels in 2016. How- ever, the agreement with the FARC can serve as a blueprint only to a limited extent, not just because of the different historical origins of the two guerrilla groups but also owing to the strongly decentralized internal structure of the ELN. The issues of a ceasefire and\n",
            "Actual Summary: Report on peace negotiations\n",
            "Generated Summary: Report on the peace process between the Colombian government and the armed group ELN (Ejército de Liberación Nacional) (peace agreement with the FARC; ELN's role in the armed conflict\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Original Text: COMMUNITY-BASED PROTECTION PREVENTION AND RESPONSE TO GBV CHILD PROTECTION ACCESS TO INTERNATIONAL PROTECTION Breakdown of Beneficiaries by Gender* Female 56.3% 0.6% Male 43.1% Gender Non-Confirm… Individuals with specific needs/vulnerabilities identified and assessed Perso ns w ith other vu … Gender Based Violence Serious m edical co nditio n Children at ris k Documentation Perso ns w ith disabiliti es No Protectio n Risk Single Parent LGBTI Elderly at Risk Tra ffic kin g in perso n 88,084 26,130 25,154 24,195 10,261 9,324 5,529 3,621 2,261 1,697 214 as of August 2022 *The number of beneficiaries reached is calculated by summing indicators selected by the sectors; there may be duplications. Turkey Protection Sector Achievements Sources: ActivityInfo, Protection Sector Legend % of 3RP Progress % of Gap against 3RP Targets Reporting Agencies 1,247,332 # of Beneficiaries by Location © 2022 TomTom, © 2022 Microsoft© 2022 TomTom, © 2022 Microsoft CorporationCorporation 31% 69%51% 49%37% 63%56% 44% 100%55% 45% 70%\n",
            "Actual Summary: Infographic on the protection sector\n",
            "Generated Summary: Infographic on the protection sector (as of August 2022)</div><div class=\"card-text card-text-description\">protection</div><p>as well as access to international protection</div>\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Original Text: Briefing Notes Summary Vietnam – July to December 2022 1 Briefing Notes Summary Group 62 – Information Centre for Asylum and Migration Vietnam – July to December 2022 01 January 2023 18 July 2022 Activist arrested On 05.07.22, the well-known democracy activist and blogger Nguyen Lan Thang was arrested in Hanoi based on Article 117 of the Criminal Code (dissemination of anti-state information). In the past, he campaigned including for the development of civil society in the country and participated in protestsagainst China's actions in the South China Sea. Blogger sentenced to prison A court in Ha Tinh province sentenced a blogger to five years and six months imprisonment on 13.07.22, again based on Art. 117 of the Criminal Code. He wrote Facebook posts about human rights violations by the Vietnamese government and was arrested in January 2022. 25 July 2022 Members of an independent religious community sentenced to several years in prison A court in the southern district of Duc Hoa (Long An Province) sentenced\n",
            "Actual Summary: Summary of recent developments (July - December 2022)\n",
            "Generated Summary: Summary of recent developments (July - December 2022) Activist arrested, blogger sentenced to prison, members of an independent religious community sentenced to several years in prison\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Original Text: Press releases Special Procedures Türkiye: UN experts call for release and end of judicial harassment of anti-torture expert 08 November 2022 Share GENEVA (8 November 2022) – UN experts* today called on Türkiye to stop using counter-terrorism legislation to intimidate human rights defenders and immediately release recently detained Sebnem Korur Fincancı, a recognised forensic medical practitioner and anti-torture expert. Fincancı, who helped develop the United Nations’ reference standards on the investigation and documentation of torture cases (the Istanbul Protocol), was arrested at her house on 26 October on unclear grounds, believed to be in retaliation for her public comments calling for investigations into the alleged use of chemical weapons and associated deaths by the Turkish military. “Dr. Fincancı’s arrest appears part of a deliberate pattern of applying counter-terrorism legislation to discredit human rights defenders and organisations and interrupt their vital human rights and medical work,” the ex\n",
            "Actual Summary: UN experts call on Turkish government to stop the use of anti-terrorism laws against human rights defenders and to release a detained forensic medical practitioner and anti-torture expert\n",
            "Generated Summary: UN experts call for the release of an anti-torture expert who has been arrested on unclear grounds for calling for investigations into the alleged use of chemical weapons by the military\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Original Text: October 30, 2022 Quarterly Report to Congress Special Inspector General for Afghanistan ReconstructionSIGAR OCT 30 2022 QUARTERLY REPORT TO THE UNITED STATES CONGRESS The National Defense Authorization Act for FY 2008 (Pub. L. No. 110- 181) established the Special Inspector General for Afghanistan Reconstruction (SIGAR). SIGAR’s oversight mission, as defined by the legislation, is to provide for the independent and objective • conduct and supervision of audits and investigations relating to the programs and operations funded with amounts appropriated or otherwise made available for the reconstruction of Afghanistan. • leadership and coordination of, and recommendations on, policies designed to promote economy, efficiency, and effectiveness in the administration of the programs and operations, and to prevent and detect waste, fraud, and abuse in such programs and operations. • means of keeping the Secretary of State and the Secretary of Defense fully and currently informed about problems and deficiencies relat\n",
            "Actual Summary: Quarterly report on US reconstruction efforts (1 July to 30 September 2022) (restrictions on the media by the Taliban; reconstruction; oversight; security situation; governance; economic and social developments)\n",
            "Generated Summary: Quarterly report on progress in the completion of the reconstruction of Afghanistan (political developments; security situation; human rights situation; economic situation; other topics)\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from transformers import pipeline\n",
        "from datasets import load_from_disk\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN_10k'\n",
        "tokenizer_path = '/content/drive/MyDrive/ACCORD_Summarisation/tokenizer'\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "test_datasets = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_10k\")\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "test_samples = test_datasets['test'].select(range(5))\n",
        "\n",
        "def extract_text(sample, field_name):\n",
        "    return sample[field_name]\n",
        "\n",
        "def decode_text(sample, max_sequence_length=1024):\n",
        "    input_text = extract_text(sample, 'input_text')\n",
        "    target_text = extract_text(sample, 'target_text')\n",
        "\n",
        "    # Truncating or splitting the text if it's too long\n",
        "    if len(input_text) > max_sequence_length:\n",
        "        input_text = input_text[:max_sequence_length]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "for sample in test_samples:\n",
        "    original_text, actual_summary = decode_text(sample)\n",
        "    generated_summary = summarizer(original_text, max_length=50, min_length=30, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
        "\n",
        "    print(f\"Original Text: {original_text}\")\n",
        "    print(f\"Actual Summary: {actual_summary}\")\n",
        "    print(f\"Generated Summary: {generated_summary}\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laTbrPyC5HvX"
      },
      "source": [
        "##Testing on given links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K827B_mCeoqZ"
      },
      "outputs": [],
      "source": [
        "# Load the trained BART model from the specified directory\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "trained_model = BartForConditionalGeneration.from_pretrained('/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN_10k')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSn-Bxw9gAk1"
      },
      "outputs": [],
      "source": [
        "#Summarization from a link\n",
        "%pip install readability-lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSe7gqbNhOX2",
        "outputId": "b8e77936-69cc-4935-e4a6-d9cd298c5596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n",
            "Generated Summary: Yevgenia Kara-Murza says her husband's placement in solitary confinement is \"torture\" The 42-year-old was initially arrested in April 2022 after returning to\n"
          ]
        }
      ],
      "source": [
        "#Summarization from a link\n",
        "from readability import Document\n",
        "import requests\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "def fetch_article_content(url):\n",
        "    response = requests.get(url)\n",
        "    print(response.status_code)\n",
        "    if response.status_code == 200:\n",
        "        doc = Document(response.text)\n",
        "        return doc.summary()\n",
        "    else:\n",
        "        return \"Error: Unable to fetch article.\"\n",
        "tokenizer_path = '/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN/tokenizer'\n",
        "tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n",
        "article_url = \"https://www.ecoi.net/de/dokument/2100746.html\"\n",
        "article_content = fetch_article_content(article_url)\n",
        "\n",
        "inputs = tokenizer(article_content, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "summary_ids = trained_model.generate(inputs['input_ids'], max_length=60, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Summary:\", summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZn9Y-kYyqLh",
        "outputId": "49d99a28-468f-45b8-ac81-bb7bc06efef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Summary: Infographic on the return of asylum seekers and returnees to Afghanistan (as of 24 November 2023)\n"
          ]
        }
      ],
      "source": [
        "#Summarization from a file\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "tokenizer_path = '/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN/tokenizer'\n",
        "tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n",
        "trained_model_path = '/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN'\n",
        "trained_model = BartForConditionalGeneration.from_pretrained(trained_model_path)\n",
        "\n",
        "# Read content from the provided text file\n",
        "file_path = '/content/CORE_Pakistan-Afghanistan.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    article_content = file.read()\n",
        "\n",
        "inputs = tokenizer(article_content, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "summary_ids = trained_model.generate(inputs['input_ids'], max_length=60, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Summary:\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C35gpOm2QYkY"
      },
      "outputs": [],
      "source": [
        "#summarization from PDF\n",
        "%pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M3m8qaEQXEp",
        "outputId": "2ab369f9-8556-477b-f8f5-6691c57bbf28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Summary: Northwestern Syria: Report on the health care situation\n"
          ]
        }
      ],
      "source": [
        "#summarization from PDF\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import fitz\n",
        "\n",
        "tokenizer_path = '/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN/tokenizer'\n",
        "tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n",
        "trained_model_path = '/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN_1k'\n",
        "trained_model = BartForConditionalGeneration.from_pretrained(trained_model_path)\n",
        "\n",
        "pdf_file_path = '/content/SyriaLearningPaper-June2022.pdf'\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc[page_num]\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "article_content = extract_text_from_pdf(pdf_file_path)\n",
        "\n",
        "inputs = tokenizer(article_content, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "summary_ids = trained_model.generate(inputs['input_ids'], max_length=60, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Summary:\", summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD8gco_P2627"
      },
      "source": [
        "##Generate Summaries for Rouge Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMqhaJsBKCFd",
        "outputId": "18b93d88-1fa0-442c-ce49-dcc1d800a2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries have been written to: /content/drive/MyDrive/ACCORD_Summarisation/generated_summaries_1k.txt\n"
          ]
        }
      ],
      "source": [
        "#generate summaries from testset\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from datasets import load_from_disk\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ACCORD_Summarisation/bart-summarization-model_EN_1k'\n",
        "tokenizer_path = '/content/drive/MyDrive/ACCORD_Summarisation/tokenizer'\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(tokenizer_path)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "test_datasets = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_1k\")\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def extract_text(sample, field_name):\n",
        "    return sample[field_name]\n",
        "\n",
        "def decode_text(sample, max_sequence_length=1024):\n",
        "    input_text = extract_text(sample, 'input_text')\n",
        "    target_text = extract_text(sample, 'target_text')\n",
        "\n",
        "    if len(input_text) > max_sequence_length:\n",
        "        input_text = input_text[:max_sequence_length]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "output_file_path = '/content/drive/MyDrive/ACCORD_Summarisation/generated_summaries_1k.txt'\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    for sample in test_datasets['test']:\n",
        "        original_text, actual_summary = decode_text(sample)\n",
        "        generated_summary = summarizer(original_text, max_length=50, min_length=10, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
        "        output_file.write(f\"{generated_summary}\\n\")\n",
        "\n",
        "print(f\"Summaries have been written to: {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QqbgPgUr8WFf"
      },
      "outputs": [],
      "source": [
        "#BART-CNN API Übersetzungen\n",
        "from transformers import pipeline, BartTokenizer, BartForConditionalGeneration\n",
        "from datasets import load_from_disk\n",
        "\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "test_datasets = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN\")\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def extract_text(sample, field_name):\n",
        "    return sample[field_name]\n",
        "\n",
        "def decode_text(sample, max_sequence_length=1024):\n",
        "    input_text = extract_text(sample, 'input_text')\n",
        "    target_text = extract_text(sample, 'target_text')\n",
        "\n",
        "    if len(input_text) > max_sequence_length:\n",
        "        input_text = input_text[:max_sequence_length]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "output_file_path = '/content/drive/MyDrive/ACCORD_Summarisation/API_generated_summaries.txt'\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "\n",
        "    for sample in test_datasets['test']:\n",
        "        original_text, actual_summary = decode_text(sample)\n",
        "        generated_summary = summarizer(original_text, max_length=50, min_length=10, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
        "        output_file.write(f\"{generated_summary}\\n\")\n",
        "\n",
        "print(f\"Summaries have been written to: {output_file_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfYmOhrt7MY5"
      },
      "source": [
        "##Compute Rouge Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhjAr7qeGTwj",
        "outputId": "45e7fd5a-f305-4c84-a274-15d7d47436a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores:\n",
            "{'rouge-1': {'r': 0.5577604240189546, 'p': 0.5968643454498719, 'f': 0.5629759507293809}, 'rouge-2': {'r': 0.42006011755737765, 'p': 0.44845884964837845, 'f': 0.4230178199871783}, 'rouge-l': {'r': 0.5355822147166035, 'p': 0.5731556141227195, 'f': 0.5408601846384697}}\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "from datasets import load_from_disk\n",
        "import json\n",
        "\n",
        "def read_generated_summaries(txt_file):\n",
        "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "        summaries = [line.strip() for line in f]\n",
        "    return summaries\n",
        "\n",
        "def compute_rouge_scores(original_summaries, generated_summaries):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(generated_summaries, original_summaries, avg=True)\n",
        "    return scores\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_dataset = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_1k/test\")\n",
        "\n",
        "    original_summaries = test_dataset[\"target_text\"]\n",
        "\n",
        "    generated_summaries = read_generated_summaries('/content/drive/MyDrive/ACCORD_Summarisation/generated_summaries_1k.txt')\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge_scores = compute_rouge_scores(original_summaries, generated_summaries)\n",
        "\n",
        "    print(\"ROUGE Scores:\")\n",
        "    print(rouge_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxffw7Wr3nA0"
      },
      "source": [
        "#Second Model Training:\n",
        "#**Fine-tuning mT5 small**\n",
        "\n",
        "https://huggingface.co/google/mt5-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3xLvB1_3sal"
      },
      "outputs": [],
      "source": [
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset, load_from_disk\n",
        "\n",
        "split_datasets = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_10k\")\n",
        "\n",
        "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-small')\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"input_text\"], max_length=1024, truncation=True, padding='max_length')\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"target_text\"], max_length=60, truncation=True, padding='max_length')\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = split_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/ACCORD_Summarisation/mt5_summarization_10k',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/content/drive/MyDrive/ACCORD_Summarisation/mt5_summarization_10k/logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=4,\n",
        "    save_total_limit=5\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model('/content/drive/MyDrive/ACCORD_Summarisation/mt5_summarization_10k')\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "results = trainer.evaluate(tokenized_datasets['test'])\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEWRYtcjBM5q"
      },
      "outputs": [],
      "source": [
        "#download tokenizer\n",
        "from datasets import load_from_disk\n",
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model_name = 'google/mt5-small'\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/ACCORD_Summarisation/mt5_summarization_10k/tokenizer')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWb8VP9UCX9S",
        "outputId": "8801848c-cd1a-4157-89d9-5d8fb228fc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Peace R esearch Institute O slo (PR IO ) PO Box 9229 G rønland, N O -0134 O slo, N orw ay V isiting A ddress: H ausm anns gate 3 w w w .prio.org Facebook: PR IO .org Tw itter: PR IO U pdates ISBN : 978-82-343-0350-0 (print) 978-82-343-0351-7 (online) C over: President K iir and M achar discuss political issues. Photo: U N M ISS via Flickr / C C BY-N C -N D Eli Stamnes Norwegian Institute of International Affairs (NUPI) Cedric de Coning Norwegian Institute of International Affairs (NUPI) Peace R esearch Institute O slo (PR IO ) PO Box 9229 G rønland, N O -0134 O slo, N orw ay V isiting A ddress: H ausm anns gate 3 w w w .prio.org Facebook: PR IO .org Tw itter: PR IO U pdates FAIR CASE BRIEF 06 The Revitalised Agreement on the Resolution of the Conflict in the Republic of South Sudan (R-ARCSS) Peace Research Institute Oslo (PRIO) Hausmanns gate 3 PO Box 9229 Grønland NO-0134 Oslo, Norway Tel +47 22 54 77 00 www.prio.org The Peace Research Institute Oslo (PRIO) is a non- profit institute established in 1959. The\n",
            "Actual Summary: Brief report on the agreement on the resolution of the conflict\n",
            "Generated Summary: <extra_id_0> -N C -N C -N C -N D Eli Stamnes Norwegian Institute of International Affairs (NUPI)\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Original Text: In Chechnya, the police detained a Grozny man who confessed to being homosexual, human rights defender Igor Kochetkov reported and called on the Prosecutor’s Office to conduct an investigation. After the detention of a young man in Chechnya, two other persons disappeared, whose contacts could be in his mobile phone, the “SK SOS” Crisis Group reported. The “Caucasian Knot” has reported that in January, the Parliamentary Assembly of the Council of Europe (PACE) adopted a resolution on protecting the rights of LGBT people in Europe, paying special attention to persecution based on sexual orientation in Chechnya, and demanded from the Russian authorities to protect the victims and punish the guilty persons. A man has been detained in Chechnya who confessed to non-traditional sexual orientation, Igor Kochetkov, the former head of the “Russian LGBT Network”*, reported on Facebook**. “Where he is now and whether he is alive at all is unknown. All this week I have been receiving messages from Chechnya asking for help\n",
            "Actual Summary: Chechnya: Man arrested after confessing his homosexuality\n",
            "Generated Summary: <extra_id_0> of Igor Kochetkov reported on Facebook**. The “Russian LGBT Network”* reported on Facebook**. <extra_id_26>**********************\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Original Text: A court sentenced a resident of Vladikavkaz to a fine of 30,000 roubles, recognizing his posts on a social media outlet as discrediting the Russian Armed Forces, the police reported. The “Caucasian Knot” has reported that after administrative responsibility for discrediting the Russian Armed Forces had been introduced on March 4, residents of Russian regions, including North Ossetia, began to be persecuted under that article as well. The court sentenced the 28-year-old resident of Vladikavkaz to a fine of 30,000 roubles, recognizing that the man posted photos on his social media page that discredited the Russian Armed Forces and also “spoke out negatively about the attitude” towards Russian citizens, the press service for the Transport Department of the Ministry of Internal Affairs (MIA) for the North-Caucasian Federal District (NCFD) reported. “The transport police warns that a repeated similar offense entails criminal liability under Article 280.3 of the Criminal Code of the Russian Federation ‘Public actio\n",
            "Actual Summary: Vladikavkaz: Resident sentenced to a fine of 30,000 Roubles for discredititing the Russian armed forces in a social media post\n",
            "Generated Summary: <extra_id_0> of the Russian Armed Forces (CRQ) <extra_id_36> the Russian Armed Forces (CRQ’s Criminal Code of the Russian Federation)’s Article 280.3 of the Criminal Code of the Russian Federation (CRQ\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Original Text: “The whole family was in the field working when the shooting started. We ran away and walked for three hours to Rumangabo in the rain,” says Ponsie Benda. “We couldn’t get back to the house. We left with what we had on us.”When clashes between the M23 armed group and the army of Democratic Republic of Congo (DRC) came close to his village, the father of 13 children found refuge in Virunga National Park’s primary school in Rumangabo, North Kivu province, DRC, in June. 190,000 people in need Like Ponsie, more than 190,000 people have had to flee their homes since late March 2022https://reliefweb.int/report/democratic-republic-congo/democratic-republic-congo-north-kivu-overview-rutshuru-nyiragongo-crisis-july-11-2022 in the territories of Rutshuru and Nyiragongo, in North Kivu province, after the resurgence of the M23 armed group and the intermittent clashes with the Congolese army. Most people have gathered along the national road linking Rutshuru to Goma, the capital of North Kivu, often in overcrowded sites.“\n",
            "Actual Summary: North Kivu province: More than 190,000 people have fled from armed conflict since March 2022; IDPs are confronted with inadequate living conditions\n",
            "Generated Summary: <extra_id_0> of the M23 armed group and the armed group and the army of Democratic Republic of Congo (the Republic of Congo ( the Republic of Congo ( the Republic of Congo ( the Republic of Congo.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Original Text: Lithuania (Tier 1) The Government of Lithuania fully meets the minimum standards for the elimination of trafficking. The government continued to demonstrate serious and sustained efforts during the reporting period, considering the impact of the COVID-19 pandemic on its anti-trafficking capacity; therefore Lithuania remained on Tier 1. These efforts included convicting significantly more traffickers; allocating more funds to NGOs for victim assistance and toward implementation of the national action plan (NAP); and identifying and assisting more trafficking victims. Furthermore, the government adopted a law on assistance to victims of crime, including trafficking, ensuring victims received assistance before, during, and after criminal proceedings. In addition, the government trained child rights specialists on identifying child trafficking victims. Although the government meets the minimum standards, authorities prosecuted fewer suspected traffickers and inconsistently implemented victim identification and re\n",
            "Actual Summary: Annual report on trafficking in persons (covering April 2021 to March 2022)\n",
            "Generated Summary: <extra_id_0> of trafficking (Tier 1) (Tier 1)) (Tier 1))) (Tier 1))) (Tier 1))) (Tier 1))) (Tier 1))). <extra_id_8>) e) e) )\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#testing the output\n",
        "from datasets import load_from_disk\n",
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ACCORD_Summarisation/mt5_summarization_1k'\n",
        "tokenizer_path = '/content/drive/MyDrive/ACCORD_Summarisation/mt5_summarization_10k/tokenizer'\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "test_datasets = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_1k\")\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "test_samples = test_datasets['test'].select(range(5))\n",
        "\n",
        "def extract_text(sample, field_name):\n",
        "    return sample[field_name]\n",
        "\n",
        "def decode_text(sample, max_sequence_length=1024):\n",
        "    input_text = extract_text(sample, 'input_text')\n",
        "    target_text = extract_text(sample, 'target_text')\n",
        "\n",
        "    if len(input_text) > max_sequence_length:\n",
        "        input_text = input_text[:max_sequence_length]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "for sample in test_samples:\n",
        "    original_text, actual_summary = decode_text(sample)\n",
        "    generated_summary = summarizer(original_text, max_length=50, min_length=30, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
        "\n",
        "    print(f\"Original Text: {original_text}\")\n",
        "    print(f\"Actual Summary: {actual_summary}\")\n",
        "    print(f\"Generated Summary: {generated_summary}\")\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvxr6wGY36sO"
      },
      "source": [
        "##Generate Summaries for Rouge Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK_QCZAe3_m-"
      },
      "outputs": [],
      "source": [
        "#Generate Summaries from Testset\n",
        "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_from_disk\n",
        "\n",
        "model_name = \"/content/drive/MyDrive/ACCORD_Summarisation/mt5_summarization_10k\"\n",
        "tokenizer_name = \"/content/drive/MyDrive/ACCORD_Summarisation/mt5_summarization_10k/tokenizer\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(tokenizer_name)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "test_datasets = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_10k\")\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def extract_text(sample, field_name):\n",
        "    return sample[field_name]\n",
        "\n",
        "def decode_text(sample, max_sequence_length=1024):\n",
        "    input_text = extract_text(sample, 'input_text')\n",
        "    target_text = extract_text(sample, 'target_text')\n",
        "\n",
        "    if len(input_text) > max_sequence_length:\n",
        "        input_text = input_text[:max_sequence_length]\n",
        "\n",
        "    return input_text, target_text\n",
        "\n",
        "output_file_path = '/content/drive/MyDrive/ACCORD_Summarisation/MT5_generated_summaries_10k.txt'\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "\n",
        "    for sample in test_datasets['test']:\n",
        "        original_text, actual_summary = decode_text(sample)\n",
        "\n",
        "        generated_summary = summarizer(original_text, max_length=50, min_length=10, length_penalty=2.0, num_beams=4)[0]['summary_text']\n",
        "\n",
        "        output_file.write(f\"{generated_summary}\\n\")\n",
        "\n",
        "print(f\"Summaries have been written to: {output_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAEPdba733sj"
      },
      "source": [
        "##Compute Rouge Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISMmI-yG31nN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f841f3e-c328-4249-ffc4-f718126f5826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores:\n",
            "{'rouge-1': {'r': 0.11530526005268565, 'p': 0.13208413513133194, 'f': 0.11751626333883382}, 'rouge-2': {'r': 0.024128702079940473, 'p': 0.02333621444202798, 'f': 0.022826149352379324}, 'rouge-l': {'r': 0.10732885722845674, 'p': 0.12337510382447453, 'f': 0.10936835437763527}}\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "from datasets import load_from_disk\n",
        "import json\n",
        "\n",
        "def read_generated_summaries(txt_file):\n",
        "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
        "        summaries = [line.strip() for line in f]\n",
        "    return summaries\n",
        "\n",
        "def compute_rouge_scores(original_summaries, generated_summaries):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(generated_summaries, original_summaries, avg=True)\n",
        "    return scores\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_dataset = load_from_disk(\"/content/drive/MyDrive/ACCORD_Summarisation/split_datasets_EN_10k/test\")\n",
        "    original_summaries = test_dataset[\"target_text\"]\n",
        "    generated_summaries = read_generated_summaries('/content/drive/MyDrive/ACCORD_Summarisation/MT5_generated_summaries_10k.txt')\n",
        "\n",
        "    rouge_scores = compute_rouge_scores(original_summaries, generated_summaries)\n",
        "\n",
        "    print(\"ROUGE Scores:\")\n",
        "    print(rouge_scores)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi3+kNb7TQCqyJtcUBLaPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}